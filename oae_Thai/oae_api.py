#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
OAE CKAN ‚Üí group=production ‚Üí AUTO:
1) Inspect & fetch all
2) Slim to 8 cols: id, year, province, commod, subcommod, atrriburte, value, unit
3) Clean
4) Keep ONLY rows where atrriburte in {"production", "value"}
5) Save per-resource + combined (CSV & Parquet)
6) PRUNE: keep only ALL_*.csv and ALL_*.parquet (‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏∑‡πà‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô outdir)

Usage:
  pip install requests pandas pyarrow openpyxl
  python oae_auto_production_slim.py --outdir oae_prod_slim
"""

import os, io, csv, json, time, argparse, re, unicodedata
from datetime import datetime
from typing import Dict, List, Any, Optional

import requests
import pandas as pd

CKAN = "https://catalog.oae.go.th/api/3/action"
TARGET_ORDER = ["id", "year", "province", "commod", "subcommod", "atrriburte", "value", "unit"]
FINAL_ATTR_SET = {"production", "value"}  # ‚¨ÖÔ∏è ‡∏Ñ‡∏á‡πÑ‡∏ß‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ 2 ‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ

# ---------------------- Helpers ----------------------

def safe_filename(s: str) -> str:
    keep = "-_()[]{} ."
    s = (s or "").strip()
    s = "".join(ch if ch.isalnum() or ch in keep else "_" for ch in s)
    return s or "file"

def http_get(url: str, params: dict=None, timeout: int=60, retries: int=3, backoff: float=1.6):
    for i in range(retries):
        try:
            r = requests.get(url, params=params, timeout=timeout)
            r.raise_for_status()
            return r
        except Exception:
            if i == retries - 1:
                raise
            time.sleep(backoff ** (i + 1))

def package_search_by_group(group="production", rows=500, start=0, q=None) -> Dict[str, Any]:
    params = {"fq": f"groups:{group}", "rows": rows, "start": start}
    if q:
        params["q"] = q
    r = http_get(f"{CKAN}/package_search", params=params)
    data = r.json()
    if not data.get("success"):
        raise RuntimeError(data)
    return data["result"]

def iter_all_datasets_in_group(group="production", q=None):
    start = 0
    while True:
        result = package_search_by_group(group=group, rows=500, start=start, q=q)
        results = result.get("results", [])
        if not results:
            break
        for ds in results:
            yield ds
        start += len(results)
        if start >= result.get("count", 0):
            break

def datastore_fields(resource_id: str) -> List[Dict[str, Any]]:
    params = {"resource_id": resource_id, "limit": 0}
    r = http_get(f"{CKAN}/datastore_search", params=params)
    data = r.json()
    if not data.get("success"):
        raise RuntimeError(data)
    fields = data["result"].get("fields", [])
    if not fields:
        r = http_get(f"{CKAN}/datastore_search", params={"resource_id": resource_id, "limit": 1})
        data = r.json()
        fields = data["result"].get("fields", [])
    return fields

def datastore_fetch_all(resource_id: str, page_size: int=50000, filters: Optional[dict]=None) -> List[Dict[str, Any]]:
    all_records = []
    offset = 0
    while True:
        params = {"resource_id": resource_id, "limit": page_size, "offset": offset}
        if filters:
            params["filters"] = json.dumps(filters, ensure_ascii=False)
        r = http_get(f"{CKAN}/datastore_search", params=params)
        data = r.json()
        if not data.get("success"):
            raise RuntimeError(data)
        res = data["result"]
        recs = res.get("records", [])
        all_records.extend(recs)
        total = res.get("total", len(all_records))
        offset += len(recs)
        if len(recs) == 0 or offset >= total:
            break
    return all_records

# ---------------------- Column mapping ----------------------

def norm(s: str) -> str:
    return unicodedata.normalize("NFC", (s or "")).strip().lower()

ALIASES = {
    "id": {"id", "_id", "recordid", "record_id", "‡∏•‡∏≥‡∏î‡∏±‡∏ö", "‡∏£‡∏´‡∏±‡∏™"},
    "year": {"year", "‡∏õ‡∏µ", "‡∏û.‡∏®.", "‡∏û‡∏®", "year_be", "be_year", "‡∏õ‡∏µ(‡∏û.‡∏®.)", "‡∏õ‡∏µ‡∏û.‡∏®.", "‡∏õ‡∏µ‡∏û‡∏®"},
    "province": {"province", "‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î", "prov", "prov_name", "province_name"},
    "commod": {"commod", "commodity", "‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤", "‡∏ä‡∏ô‡∏¥‡∏î‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤", "‡∏û‡∏∑‡∏ä", "‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏©‡∏ï‡∏£"},
    "subcommod": {"subcommod", "sub_commod", "subcommodity", "‡∏ä‡∏ô‡∏¥‡∏î‡∏¢‡πà‡∏≠‡∏¢", "‡∏û‡∏±‡∏ô‡∏ò‡∏∏‡πå", "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏¢‡πà‡∏≠‡∏¢"},
    "atrriburte": {"atrriburte", "attribute", "attr", "‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£", "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£", "‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î"},
    "value": {"value", "val", "‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ß‡∏±‡∏î‡πÑ‡∏î‡πâ", "‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì", "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï", "‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï", "‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤"},
    "unit": {"unit", "units", "‡∏´‡∏ô‡πà‡∏ß‡∏¢", "‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏ô‡∏±‡∏ö"},
}

def build_col_map(columns: List[str]) -> Dict[str, str]:
    norm_cols = {norm(c): c for c in columns}
    mapping = {}
    for target, cands in ALIASES.items():
        hit = None
        for cand in cands:
            if norm(cand) in norm_cols:
                hit = norm_cols[norm(cand)]
                break
        if not hit:
            for nc, orig in norm_cols.items():
                if any(nc.startswith(norm(c)) for c in cands):
                    hit = orig
                    break
        if not hit:
            for nc, orig in norm_cols.items():
                if any((norm(c) in nc and len(norm(c)) >= 3) for c in cands):
                    hit = orig
                    break
        if hit:
            mapping[target] = hit
    return mapping

def slim_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame(columns=TARGET_ORDER)
    cols = list(df.columns)
    cmap = build_col_map(cols)
    present_targets = [t for t in TARGET_ORDER if t in cmap]
    if not present_targets:
        return pd.DataFrame(columns=TARGET_ORDER)
    slim = df[[cmap[t] for t in present_targets]].copy()
    slim.columns = present_targets
    for t in TARGET_ORDER:
        if t not in slim.columns:
            slim[t] = pd.NA
    slim = slim[TARGET_ORDER]
    return slim

# ---------------------- Cleaning ----------------------

WS_RE = re.compile(r"\s+")
NUM_RE = re.compile(r"[^0-9\-\.,]")

UNIT_MAP = {
    "‡∏Å‡∏Å.": "‡∏Å‡∏¥‡πÇ‡∏•‡∏Å‡∏£‡∏±‡∏°",
    "‡∏Å‡∏Å": "‡∏Å‡∏¥‡πÇ‡∏•‡∏Å‡∏£‡∏±‡∏°",
    "kg": "‡∏Å‡∏¥‡πÇ‡∏•‡∏Å‡∏£‡∏±‡∏°",
    "‡∏ï‡∏±‡∏ô": "‡∏ï‡∏±‡∏ô",
    "t": "‡∏ï‡∏±‡∏ô",
    "‡∏û‡∏±‡∏ô‡∏ï‡∏±‡∏ô": "‡∏ï‡∏±‡∏ô",  # Will be converted to ‡∏ï‡∏±‡∏ô with value * 1000
    "‡∏ö‡∏≤‡∏ó": "‡∏ö‡∏≤‡∏ó",
    "bt": "‡∏ö‡∏≤‡∏ó",
    "baht": "‡∏ö‡∏≤‡∏ó",
    "‡πÑ‡∏£‡πà": "‡πÑ‡∏£‡πà",
    "‡∏ï‡∏±‡∏ô/‡πÑ‡∏£‡πà": "‡∏ï‡∏±‡∏ô/‡πÑ‡∏£‡πà",
    "‡∏Å‡∏¥‡πÇ‡∏•‡∏Å‡∏£‡∏±‡∏°/‡πÑ‡∏£‡πà": "‡∏Å‡∏¥‡πÇ‡∏•‡∏Å‡∏£‡∏±‡∏°/‡πÑ‡∏£‡πà",
    "‡∏•‡∏ö.‡∏°.": "‡∏•‡∏π‡∏Å‡∏ö‡∏≤‡∏®‡∏Å‡πå‡πÄ‡∏°‡∏ï‡∏£",
    "‡∏•‡∏ö‡∏°.": "‡∏•‡∏π‡∏Å‡∏ö‡∏≤‡∏®‡∏Å‡πå‡πÄ‡∏°‡∏ï‡∏£",
}

def convert_value_by_unit(value: float, unit: str) -> tuple[float, str]:
    """Convert value based on unit and return new value and unit."""
    if pd.isna(value):
        return value, unit
    
    unit = clean_text(unit).lower()  # Use clean_text to normalize the unit string
    if "‡∏û‡∏±‡∏ô‡∏ï‡∏±‡∏ô" in unit:  # Check if unit contains ‡∏û‡∏±‡∏ô‡∏ï‡∏±‡∏ô
        return float(value) * 1000, "‡∏ï‡∏±‡∏ô"
    return value, UNIT_MAP.get(unit, unit)

# ‚¨áÔ∏è ‡∏ó‡∏≥‡πÉ‡∏´‡πâ "‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤" ‡∏ñ‡∏π‡∏Å‡πÅ‡∏°‡∏õ‡πÄ‡∏õ‡πá‡∏ô value (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç FINAL_ATTR_SET)
ATTR_MAP = {
    "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï": "production",
    "‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡∏ï‡πà‡∏≠‡πÑ‡∏£‡πà": "yield_per_rai",
    "‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏≤‡∏∞‡∏õ‡∏•‡∏π‡∏Å": "planted_area",
    "‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß": "harvested_area",
    "‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏¢‡∏∑‡∏ô‡∏ï‡πâ‡∏ô": "perennial_area",
    "‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏ú‡∏•": "bearing_area",
    "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡∏ß‡πÄ‡∏£‡∏∑‡∏≠‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡∏•‡∏π‡∏Å": "num_households",
    "‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤": "value",   # üî• ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô "value"
}

def clean_text(x: Any) -> str:
    s = unicodedata.normalize("NFC", str(x)) if pd.notna(x) else ""
    s = s.replace("\u200b", "")
    s = WS_RE.sub(" ", s).strip()
    return s

def clean_year(y: Any) -> Optional[int]:
    if pd.isna(y):
        return None
    s = clean_text(y)
    if not s:
        return None
    s = re.sub(r"[^0-9\-]", "", s)
    if not s:
        return None
    try:
        val = int(s)
    except Exception:
        return None
    if 2400 <= val <= 2600:
        val = val - 543
    if 1900 <= val <= 2100:
        return val
    return None

def clean_province(p: Any) -> str:
    s = clean_text(p)
    if not s:
        return ""
    s = re.sub(r"^\s*‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î\s*", "", s)
    s = " ".join(w[:1].upper() + w[1:] if w else "" for w in s.split(" "))
    return s

def clean_unit(u: Any) -> str:
    s = clean_text(u)
    # Return the exact string "‡∏û‡∏±‡∏ô‡∏ï‡∏±‡∏ô" if it matches case-insensitively
    if s.lower() == "‡∏û‡∏±‡∏ô‡∏ï‡∏±‡∏ô".lower():
        return "‡∏û‡∏±‡∏ô‡∏ï‡∏±‡∏ô"
    return s

def clean_attrib(a: Any) -> str:
    s = clean_text(a)
    return ATTR_MAP.get(s, s).strip().lower()

def parse_value(v: Any) -> Optional[float]:
    if pd.isna(v):
        return None
    s = clean_text(v)
    if not s:
        return None
    s = NUM_RE.sub("", s).replace(",", "")
    if s in {"", "-", ".", "-.", ".-"}:
        return None
    try:
        return float(s)
    except Exception:
        return None

def convert_value_and_unit(value: float, unit: str) -> tuple[float, str]:
    """Convert value based on unit and return the converted value and standardized unit."""
    if pd.isna(value):
        return value, unit
    
    # Check for ‡∏û‡∏±‡∏ô‡∏ï‡∏±‡∏ô with exact match
    if unit == "‡∏û‡∏±‡∏ô‡∏ï‡∏±‡∏ô":
        print(f"Converting {value} ‡∏û‡∏±‡∏ô‡∏ï‡∏±‡∏ô to {value * 1000} ‡∏ï‡∏±‡∏ô")  # Debug print
        return value * 1000, "‡∏ï‡∏±‡∏ô"
    return value, unit

def clean_slim_df(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame(columns=TARGET_ORDER)

    out = pd.DataFrame()
    out["id"] = df["id"].apply(clean_text)
    out["year"] = df["year"].apply(clean_year)
    out["province"] = df["province"].apply(clean_province)
    out["commod"] = df["commod"].apply(clean_text)
    out["subcommod"] = df["subcommod"].apply(clean_text)
    out["atrriburte"] = df["atrriburte"].apply(clean_attrib)
    
    # First clean the values and units
    out["value"] = df["value"].apply(parse_value)
    out["unit"] = df["unit"].apply(clean_unit)
    
    # Then convert values based on units (e.g., ‡∏û‡∏±‡∏ô‡∏ï‡∏±‡∏ô -> ‡∏ï‡∏±‡∏ô)
    converted_values = []
    converted_units = []
    for val, unit in zip(out["value"], out["unit"]):
        new_val, new_unit = convert_value_and_unit(val, unit)
        converted_values.append(new_val)
        converted_units.append(new_unit)
    
    out["value"] = converted_values
    out["unit"] = converted_units

    # Filter and clean up
    out = out[out["atrriburte"].isin(FINAL_ATTR_SET)].copy()
    out = out.dropna(subset=["value"]).copy()

    for c in ["province", "commod", "subcommod", "atrriburte", "unit", "id"]:
        out[c] = out[c].apply(lambda x: x if x is not None else "").astype(str).str.strip()

    out = out[TARGET_ORDER].drop_duplicates().reset_index(drop=True)
    return out

# ---------------------- PRUNE ----------------------

def prune_outdir(outdir: str, group: str, parquet_saved: bool):
    """‡∏•‡∏ö‡∏ó‡∏∏‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô outdir ‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏ß‡∏° ALL_{group}_SLIM_CLEAN_ATTR.(csv|parquet)"""
    keep = {f"ALL_{group}_SLIM_CLEAN_ATTR.csv"}
    if parquet_saved:
        keep.add(f"ALL_{group}_SLIM_CLEAN_ATTR.parquet")

    for name in os.listdir(outdir):
        path = os.path.join(outdir, name)
        if os.path.isfile(path) and name not in keep:
            try:
                os.remove(path)
            except Exception as e:
                print(f"‚ö†Ô∏è ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {name} ‚Üí {e}")

# ---------------------- Main dumping ----------------------

def dump_group_slim_clean(group="production", outdir="oae_prod_slim", include_non_datastore=True):
    os.makedirs(outdir, exist_ok=True)
    ts = datetime.utcnow().strftime("%Y%m%d-%H%M%S")

    catalog_rows = []
    combined_slim_clean = []

    print(f"üîé Inspecting group: {group}")
    for ds in iter_all_datasets_in_group(group=group):
        ds_title = ds.get("title") or ds.get("name")
        ds_name = ds.get("name")
        print(f"\nüì¶ Dataset: {ds_title} (name={ds_name})")

        resources = ds.get("resources", []) or []
        if not resources:
            print("  ‚Ä¢ ‡πÑ‡∏°‡πà‡∏°‡∏µ resources")
            continue

        for r in resources:
            rid = r.get("id")
            rname = r.get("name") or ""
            fmt = (r.get("format") or "").upper()
            url = r.get("url") or ""
            active = r.get("datastore_active", False)

            print(f"  ‚Üí Resource: {rname} [{fmt}] rid={rid} datastore_active={active}")

            field_ids = []
            if active and rid:
                try:
                    fields = datastore_fields(rid)
                    field_ids = [f.get("id") for f in fields if f.get("id")]
                except Exception as e:
                    print(f"    ‚ö†Ô∏è ‡∏≠‡πà‡∏≤‡∏ô fields ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}")

            catalog_rows.append({
                "dataset": ds_title,
                "dataset_name": ds_name,
                "resource_name": rname,
                "resource_id": rid,
                "format": fmt,
                "datastore_active": active,
                "fields": "|".join(field_ids),
                "n_fields": len(field_ids),
                "preview_api": f"{CKAN}/datastore_search?resource_id={rid}&limit=5" if active and rid else "",
                "download_url": url,
            })

            slim_df = None

            # 1) Try DataStore
            if active and rid:
                try:
                    records = datastore_fetch_all(rid, page_size=50000)
                    raw = pd.DataFrame(records)
                    slim_df = slim_dataframe(raw)
                    print(f"    ‚úÖ datastore pulled rows={len(raw)} slim_rows={len(slim_df)}")
                except Exception as e:
                    print(f"    ‚ùå datastore pull error: {e}")

            # 2) File fallback for non-datastore
            if slim_df is None and include_non_datastore and url:
                try:
                    resp = http_get(url)
                    content = resp.content
                    if fmt in {"CSV", ""} or url.lower().endswith(".csv"):
                        raw = pd.read_csv(io.BytesIO(content), dtype=str, encoding="utf-8", on_bad_lines="skip")
                    elif fmt in {"XLSX", "XLS"} or url.lower().endswith((".xlsx", ".xls")):
                        raw = pd.read_excel(io.BytesIO(content), dtype=str, engine="openpyxl")
                    else:
                        raw = None
                    if raw is not None:
                        slim_df = slim_dataframe(raw)
                        print(f"    ‚úÖ file pulled rows={len(raw)} slim_rows={len(slim_df)}")
                except Exception as e:
                    print(f"    ‚è≠Ô∏è skip file pull: {e}")

            # Save per-resource SLIM + CLEAN (with attr filter)
            if isinstance(slim_df, pd.DataFrame) and not slim_df.empty:
                slim_clean = clean_slim_df(slim_df)
                if slim_clean.empty:
                    print("    ‚ö†Ô∏è no rows after clean (attr filter or value parse)")
                    continue
                fname = f"{safe_filename(rname or ds_title)}__{(rid or '')[:8]}__SLIM_CLEAN_ATTR.csv"
                out_path = os.path.join(outdir, fname)
                slim_clean.to_csv(out_path, index=False, encoding="utf-8-sig", quoting=csv.QUOTE_MINIMAL)
                print(f"    üíæ saved: {out_path} (rows={len(slim_clean)})")
                combined_slim_clean.append(slim_clean)
            else:
                print("    ‚ö†Ô∏è no slim rows (columns not matched or empty data)")

    # save catalog (‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏•‡∏ö‡∏ó‡∏¥‡πâ‡∏á‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á‡πÇ‡∏î‡∏¢ prune)
    df_catalog = pd.DataFrame(catalog_rows)
    cat_csv = os.path.join(outdir, f"catalog_{group}_{ts}.csv")
    cat_json = os.path.join(outdir, f"catalog_{group}_{ts}.json")
    df_catalog.to_csv(cat_csv, index=False, encoding="utf-8-sig", quoting=csv.QUOTE_MINIMAL)
    df_catalog.to_json(cat_json, orient="records", indent=2, force_ascii=False)
    print(f"\nüóÇÔ∏è Catalog saved:\n   - {cat_csv}\n   - {cat_json}")

    # save combined (ONLY production/value)
    parquet_ok = False
    if combined_slim_clean:
        df_all = pd.concat(combined_slim_clean, ignore_index=True)
        df_all = df_all[TARGET_ORDER].drop_duplicates().reset_index(drop=True)
        all_csv = os.path.join(outdir, f"ALL_{group}_SLIM_CLEAN_ATTR.csv")
        all_parquet = os.path.join(outdir, f"ALL_{group}_SLIM_CLEAN_ATTR.parquet")
        df_all.to_csv(all_csv, index=False, encoding="utf-8-sig", quoting=csv.QUOTE_MINIMAL)
        try:
            df_all.to_parquet(all_parquet, index=False)
            parquet_ok = True
            print(f"üì¶ Combined saved:\n   - {all_csv} (rows={len(df_all)})\n   - {all_parquet} (rows={len(df_all)})")
        except Exception as e:
            print(f"üì¶ Combined saved:\n   - {all_csv} (rows={len(df_all)})\n   ‚ö†Ô∏è Parquet not saved (pyarrow required): {e}")
        # üî• ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏ß‡∏° ‡∏•‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô outdir
        prune_outdir(outdir, group, parquet_saved=parquet_ok)
    else:
        print("\n‚ö†Ô∏è No combined data (maybe no rows matched production/value).")

# ---------------------- CLI ----------------------

def main():
    ap = argparse.ArgumentParser(description="Auto pull OAE production ‚Üí slim 8 cols ‚Üí clean ‚Üí keep only production/value.")
    ap.add_argument("--group", default="production", help="CKAN group (default: production)")
    ap.add_argument("--outdir", default="oae_prod_slim", help="Output directory")
    ap.add_argument("--no-file-fallback", action="store_true",
                    help="Do NOT download CSV/XLSX when datastore is inactive")
    args = ap.parse_args()

    dump_group_slim_clean(
        group=args.group,
        outdir=args.outdir,
        include_non_datastore=not args.no_file_fallback
    )

if __name__ == "__main__":
    main()
